{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essais de réseaux de neurones pour le projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import time\n",
    "import numpy\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 7.0)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[item[1]] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[val[1]]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.Y)\n",
    "    weights = make_weights_for_balanced_classes(dataset.X, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cmath\n",
    "j = cmath.sqrt(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../Data/Dataset_v0.txt\"\n",
    "if not os.path.isfile(dataset_path+\".gz\"):\n",
    "    with open(dataset_path, \"rb\") as f_in:\n",
    "        with gzip.open(dataset_path+\".gz\", \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(dataset_path, skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer un Pytorch dataset et un dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, set_type=\"train\"):\n",
    "        super().__init__()\n",
    "        # garde les paramètres en mémoire\n",
    "        self.path = path\n",
    "        # charger les données\n",
    "        data = np.loadtxt(dataset_path, skiprows=4)\n",
    "\n",
    "        # Organisation des entrées en sorties du réseau\n",
    "        self.X = data[:, 3:204]\n",
    "        \n",
    "        self.Y = data[:, 204:2206]\n",
    "        self.Y = self.Y[:, :1001] + j*self.Y[:, 1001:]\n",
    "        self.Y = 10*np.log10(np.abs(self.Y)**2)\n",
    "        \n",
    "        if set_type is \"train\":\n",
    "            self.X = self.X[:int(self.X.shape[0]/2), :]\n",
    "            self.Y = self.Y[:int(self.Y.shape[0]/2), :]\n",
    "            \n",
    "        elif set_type is \"test\":\n",
    "            self.X = self.X[int(self.X.shape[0]/2):, :]\n",
    "            self.Y = self.Y[int(self.Y.shape[0]/2):, :]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # TODO Q1A\n",
    "        # On retourne les data et l'étiquette pour index\n",
    "        return self.X[index, :], self.Y[index, :]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO Q1A\n",
    "        return self.Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO Q1B\n",
    "        # Initialiser ici les modules contenant des \n",
    "        # paramètres à optimiser. Ces modules seront\n",
    "        # utilisés dans la méthode 'forward'\n",
    "        \n",
    "        # couches de convolution\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(2, 5), stride=2,bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=2, bias=False)\n",
    "\n",
    "        \n",
    "        # Couches de normalisation\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.linear = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sélectionne la taille batch à l'entrée\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # TODO Q1B\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = F.relu(self.batch_norm4(self.conv4(x)))\n",
    "        x = F.relu(self.batch_norm5(self.conv5(x)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Fait un average pooling sur les caractéristiques\n",
    "        # de chaque filtre\n",
    "        x = x.view(batch_size, 64, -1).mean(dim=2)\n",
    "        \n",
    "        # TODO Q1B\n",
    "        # Couches lineaire et sigmoide\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CDCDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3f0532040d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creation des datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JC/Data/Dataset_v0.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JC/Data/Dataset_v0.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CDCDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Creation des datasets\n",
    "train_set = CDCDataset(\"JC/Data/Dataset_v0.txt\", \"train\")\n",
    "test_set = CDCDataset(\"JC/Data/Dataset_v0.txt\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
